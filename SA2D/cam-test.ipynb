{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot Anomaly Detection\n",
    "\n",
    "## Some short descriptions:\n",
    "\n",
    "\n",
    "- The test dataset should be prepared as follows (similar to the dataset used in training):\n",
    " \n",
    " - test_dataset (a folder that contains the test videos in the form of frame images)\n",
    "   - 0 (folder 0 represents a test scenario, one video under this folder would be enough; however, if you want to have more test videos under this folder, **these videos must be captured from the same camera view and it should not be a mixture of different scenarios**)\n",
    "     - video_frames (a folder that contains the frame images)\n",
    "   \n",
    "   ...\n",
    "   \n",
    "- The finetuning process: \n",
    "\n",
    " - a 3-frame video sequence is passed into the pre-trained model for finetuning,\n",
    " \n",
    " - the finetuned model is saved into the `model` folder,\n",
    " \n",
    " - and after that the rest frames are passed into the finetuned model for frame prediction and anomaly scoring.\n",
    "\n",
    "- Each input is a 4-frame video sequence in the form of frame images, and the first 3 frames are used for the prediction of the 4-th frame.\n",
    "\n",
    "- The predicted frame is compared with the actual frame, and if the difference between the predicted frame and the actual frame is greater than a threshold (currently we use 0.6 at this stage), show this frame.\n",
    "\n",
    "\n",
    "## Suggestions for improvements\n",
    "\n",
    "The ways to improve the performance of our model are:\n",
    "\n",
    "- The training dataset is better to have more scenarios;\n",
    "\n",
    "- the current dataset is suffering from the lower resolution.\n",
    "\n",
    "### -- Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import ast\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import random\n",
    "import torch.utils.data\n",
    "import torchvision.utils as vutils\n",
    "import torch.backends.cudnn as cudn\n",
    "from torch.nn import functional as F\n",
    "from unet_parts import *\n",
    "# from scipy.misc import imsave\n",
    "import torch.nn as nn\n",
    "import ast\n",
    "import sys\n",
    "import imageio\n",
    "# from skimage import img_as_ubyte\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import cv2\n",
    "\n",
    "from rGAN import Generator, Discriminator\n",
    "from dataset import TrainingDataset\n",
    "from utils import createEpochData, roll_axis, loss_function, create_folder, prep_data, createEpochDataTest\n",
    "\n",
    "# load functions from the training script for the finetuning of the model\n",
    "from train import Load_Dataloader, overall_generator_pass, overall_discriminator_pass, meta_update_model\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_generator_pass_test(generator, discriminator, img, gt, valid):\n",
    "    # print(len(img), gt.shape)\n",
    "    recon_batch = generator(img)\n",
    "    recon_batch = (recon_batch-recon_batch.min()) / (recon_batch.max() - recon_batch.min())\n",
    "    gt = (gt-gt.min()) / (gt.max()-gt.min())\n",
    "    msssim, f1, psnr = loss_function(recon_batch, gt)\n",
    "    # print(msssim, f1, psnr)\n",
    "    \n",
    "    imgs = recon_batch.data.cpu().numpy()[0, :]\n",
    "    imgs = roll_axis(imgs)\n",
    "    \n",
    "    return imgs, psnr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Visualization functions for ground truth and predicted frame images\n",
    "\n",
    "We first visualize the first 3 frame video sequence (for the videos used in finetuning and testing/validation stage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def frame_visualization(img):\n",
    "    for frame in range(len(img)):\n",
    "        one_img = np.squeeze(img[frame])\n",
    "        one_img = one_img.cpu()\n",
    "        one_img = np.transpose(one_img, (1, 2, 0))\n",
    "        plt.imshow(one_img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "def pred_frame_visualization(img):\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Main test functions (including finetuning and validation/testing)\n",
    "- **Before running the following codes, you should update the `frame_path` to the test dataset you prepared as I mentioned in the beginning of this jupyter notebook.**\n",
    "- the threshold value for defining the anomaly is set as 0.85 at this stage for testing purposes, this value can be set to 0.9, 1.0, etc.\n",
    "- we use K-shot = 10 to test 10 videos, to test 100 videos just change K-shot = 100 in main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- start loading pre-trained model\n",
      "- loading pretrained model done\n",
      "['01_0141', '01_0162', '01_0163', '01_0177', '02_0128', '02_0161', '02_0164', '03_0031', '03_0032', '03_0033', '03_0035', '03_0036', '03_0039', '03_0041', '03_0059', '03_0060', '03_0061', '04_0001', '04_0003', '04_0004', '04_0010', '04_0011', '04_0012', '04_0013', '04_0046', '04_0050', '05_0017', '05_0018', '05_0019', '05_0020', '05_0021', '05_0022', '05_0023', '05_0024', '06_0144', '06_0145', '06_0147', '06_0150', '06_0153', '06_0155', '07_0005', '07_0006', '07_0007', '07_0008', '07_0009', '07_0047', '07_0048', '07_0049', '08_0044', '08_0058', '08_0077', '08_0078', '08_0079', '08_0080', '08_0156', '08_0157', '08_0158', '08_0159', '08_0178', '08_0179', '09_0057', '10_0037', '10_0038', '10_0042', '10_0074', '10_0075', '11_0176', '12_0142', '12_0143', '12_0148', '12_0149', '12_0151', '12_0152', '12_0154', '12_0173', '12_0174', '12_0175']\n",
      "----------- selected videos:  ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141']\n",
      "[[[['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\000.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\001.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\002.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\003.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\001.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\002.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\003.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\004.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\002.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\003.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\004.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\005.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\003.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\004.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\005.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\006.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\004.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\005.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\006.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\007.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\005.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\006.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\007.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\008.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\006.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\007.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\008.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\009.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\007.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\008.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\009.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\010.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\008.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\009.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\010.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\011.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\009.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\010.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\011.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\012.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\010.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\011.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\012.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\013.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\011.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\012.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\013.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\014.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\012.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\013.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\014.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\015.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\013.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\014.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\015.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\016.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\014.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\015.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\016.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\017.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\015.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\016.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\017.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\018.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\016.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\017.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\018.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\019.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\017.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\018.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\019.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\020.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\018.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\019.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\020.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\021.jpg'], ['C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\019.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\020.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\021.jpg', 'C:\\\\Users\\\\liyun\\\\Desktop\\\\testing\\\\frames\\\\01_0141\\\\022.jpg']]]]\n",
      "\n",
      " Meta Validation/Test \n",
      "\n",
      "C:\\Users\\liyun\\Desktop\\testing\\frames\\01_0141\n",
      "\n",
      " Meta Validation/Test \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:10<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [310, 20]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 116\u001b[0m\n\u001b[0;32m    114\u001b[0m         y_psnr_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(normalized_psnr_list)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m#         print(normalized_psnr_list)\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m         fpr, tpr, thresholds \u001b[38;5;241m=\u001b[39m roc_curve(y_gt, y_psnr_scores)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m#         print(thresholds)\u001b[39;00m\n\u001b[0;32m    119\u001b[0m         roc_auc \u001b[38;5;241m=\u001b[39m auc(fpr, tpr)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:992\u001b[0m, in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mroc_curve\u001b[39m(\n\u001b[0;32m    905\u001b[0m     y_true, y_score, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    906\u001b[0m ):\n\u001b[0;32m    907\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;124;03m    array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 992\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m _binary_clf_curve(\n\u001b[0;32m    993\u001b[0m         y_true, y_score, pos_label\u001b[38;5;241m=\u001b[39mpos_label, sample_weight\u001b[38;5;241m=\u001b[39msample_weight\n\u001b[0;32m    994\u001b[0m     )\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1003\u001b[0m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:751\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m    749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m--> 751\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m    752\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n\u001b[0;32m    753\u001b[0m y_score \u001b[38;5;241m=\u001b[39m column_or_1d(y_score)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [310, 20]"
     ]
    }
   ],
   "source": [
    "\"\"\"TEST SCRIPT\"\"\"\n",
    "\n",
    "\n",
    "num_tasks = 1\n",
    "adam_betas = (0.5, 0.999)\n",
    "gen_lr = 2e-4\n",
    "dis_lr = 1e-5\n",
    "model_folder_path = \"model\"\n",
    "torch.manual_seed(1)\n",
    "batch_size = 1\n",
    "    \n",
    "generator = Generator(batch_size=batch_size) \n",
    "discriminator = Discriminator()\n",
    "generator.cuda()\n",
    "discriminator.cuda()\n",
    "\n",
    "\n",
    "k_shots = 1000\n",
    "\n",
    "# define dataloader\n",
    "tf = transforms.Compose([transforms.Resize((256,256)),transforms.ToTensor()])\n",
    "\n",
    "create_folder(model_folder_path)\n",
    "generator_path = os.path.join(model_folder_path, str.format(\"Generator_finetuned_liyun1500_shtech.pt\"))\n",
    "discriminator_path = os.path.join(model_folder_path, str.format(\"Discriminator_finetuned_liyun1500_shtech.pt\"))\n",
    "\n",
    "# load the pre-trained model\n",
    "print('- start loading pre-trained model')\n",
    "\n",
    "generator.load_state_dict(torch.load(generator_path))\n",
    "discriminator.load_state_dict(torch.load(discriminator_path))\n",
    "# if you use CPU\n",
    "#     generator.load_state_dict(torch.load(generator_path, map_location=torch.device('cpu')))\n",
    "#     discriminator.load_state_dict(torch.load(discriminator_path, map_location=torch.device('cpu')))\n",
    "    \n",
    "print('- loading pretrained model done')\n",
    "frame_path = r'C:\\Users\\liyun\\Desktop\\testing\\frames'\n",
    "dirs = os.listdir(frame_path)\n",
    "dirs = [d for d in dirs if d != '.DS_Store']  # Exclude '.DS_Store'\n",
    "dirs.sort(key=int)\n",
    "print(dirs)\n",
    "auc_list = []\n",
    "    \n",
    "# the test dataloader\n",
    "train_path_list = createEpochDataTest(frame_path, num_tasks, 10)\n",
    "print(train_path_list)\n",
    "train_dataloader = Load_Dataloader(train_path_list, tf, batch_size)\n",
    "\n",
    "    \n",
    "# Meta-Validation\n",
    "print ('\\n Meta Validation/Test \\n')\n",
    "\n",
    "# forward pass\n",
    "    \n",
    "for i, epoch_of_tasks in enumerate(train_dataloader):\n",
    "    epoch_results = 'results'# .format(epoch+1)\n",
    "    create_folder(epoch_results)\n",
    "#     print(epoch_of_tasks)\n",
    "        \n",
    "    for tidx, task in enumerate(epoch_of_tasks):\n",
    "        \n",
    "        print(os.path.join(frame_path, dirs[tidx]))\n",
    "        \n",
    "        frame_mask_path = r'C:\\Users\\liyun\\Desktop\\testing\\test_frame_mask'\n",
    "        npy_path = os.path.join(frame_mask_path, dirs[tidx])\n",
    "        loaded_array = np.load(npy_path + '.npy')\n",
    "        \n",
    "        s_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generator.eval()\n",
    "            discriminator.eval()\n",
    "                \n",
    "            # Meta-Validation\n",
    "            print ('\\n Meta Validation/Test \\n')\n",
    "\n",
    "            for vidx, val_frame_sequence in tqdm(enumerate(task[-k_shots:]), total = len(task[-k_shots:])):\n",
    "                # print(vidx)\n",
    "#                 print(val_frame_sequence)\n",
    "                if vidx == 0:\n",
    "                    dummy_frame_sequence = val_frame_sequence\n",
    "                        \n",
    "                if 1: # vidx % 2 == 0:\n",
    "                    img = val_frame_sequence[0]\n",
    "\n",
    "                    # frame_visualization(img)\n",
    "                    gt = val_frame_sequence[1]\n",
    "                        \n",
    "                    img, gt, valid, fake = prep_data(img, gt)\n",
    "\n",
    "                    # k-Validation Generator\n",
    "                    imgs, psnr = overall_generator_pass_test(generator, discriminator, img, gt, valid)\n",
    "                    img_path = os.path.join(epoch_results,'{}-fig-val{}.png'.format(tidx+1, vidx+1))\n",
    "                    # imsave(img_path , imgs)\n",
    "\n",
    "                    # imgs = imgs.astype(np.uint8)\n",
    "                    # imgs = (imgs-np.min(imgs))/(np.max(imgs) - np.min(imgs))\n",
    "                    imgs = cv2.normalize(imgs, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "                    # imageio.imwrite(img_path , img_as_ubyte(imgs))\n",
    "                        \n",
    "                    s_list.append(psnr)\n",
    "#                     print('Frame No.: ', vidx, ' ...')\n",
    "\n",
    "\n",
    "\n",
    "        normalized_psnr_list = []\n",
    "        y_score = np.array(s_list)\n",
    "        y_gt = loaded_array[3:]\n",
    "        print(y_gt)\n",
    "\n",
    "        for psnr in y_score:\n",
    "            normalized_psnr = 1 - (psnr - np.min(y_score)) / (np.max(y_score) - np.min(y_score))\n",
    "            normalized_psnr_list.append(normalized_psnr)\n",
    "        y_psnr_scores = np.array(normalized_psnr_list)\n",
    "#         print(normalized_psnr_list)\n",
    "        fpr, tpr, thresholds = roc_curve(y_gt, y_psnr_scores)\n",
    "#         print(thresholds)\n",
    "\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_list.append(roc_auc)\n",
    "\n",
    "        print(\"AUC:\", roc_auc)\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlabel('False Positive Rate (FPR)')\n",
    "        plt.ylabel('True Positive Rate (TPR)')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        x_values = range(len(y_psnr_scores))\n",
    "\n",
    "        plt.plot(x_values, y_psnr_scores, marker='o', linestyle='-')\n",
    "\n",
    "        plt.title('The line chart')\n",
    "        plt.xlabel('i-th frame')\n",
    "        plt.ylabel('anomaly score')\n",
    "        # plt.axhline(y=0.98, color='r', linestyle='--', label='y=0.98')\n",
    "        plt.show()\n",
    "        print(auc_list)\n",
    "\n",
    "print(auc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0.8864601018675722, 0.7176496478873239, 0.8544747433636323, 0.6998158379373849, 0.3807155322862129, 0.7834581569521328, 0.8775240384615385, 0.9263285566686134, 0.24946601941747573]\n",
    "round1 = [0.8392756083757782, 0.649452269170579, 0.8593587482476371, 0.6822702159718734, 0.36906631762652703, 0.837959947899707, 0.8686698717948719, 0.9246664570324784, 0.2219660194174757, 0.6522238163558105]\n",
    "round2 = [0.80398991057097, 0.9644807150490624, 0.6899177729464492, 0.5870643642072213, 0.35214549587951116, 0.9266185347378388, 0.8172229725384269, 0.8355593124823893, 0.8765978367748279, 0.9936588921282798]\n",
    "round3 = [0.9011524822695036, 0.8963072055855561, 0.933464696223317, 0.8169014084507042, 0.686618863761721, 0.9551117938214713, 0.7301129140806102, 0.5254385964912281, 0.9528343086732466, 0.9570776255707762]\n",
    "round4 = [0.8089291680588039, 0.8445885509838998, 0.8303990610328639, 0.8926791677369638, //\n",
    "          0.5881070249882132, 0.23456937799043062, 0.39896616541353386, 0.3704577968526466, 0.5126456876456876, 0.34292084726867333]\n",
    "\n",
    "mean_a = sum(a) / len(a)\n",
    "mean_b = sum(round3) / len(round3)\n",
    "\n",
    "print(\"Mean of list a:\", mean_a)\n",
    "print(\"Mean of list b:\", mean_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
