{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot Anomaly Detection\n",
    "\n",
    "## Some short descriptions:\n",
    "\n",
    "\n",
    "- The test dataset should be prepared as follows (similar to the dataset used in training):\n",
    " \n",
    " - test_dataset (a folder that contains the test videos in the form of frame images)\n",
    "   - 0 (folder 0 represents a test scenario, one video under this folder would be enough; however, if you want to have more test videos under this folder, **these videos must be captured from the same camera view and it should not be a mixture of different scenarios**)\n",
    "     - video_frames (a folder that contains the frame images)\n",
    "   \n",
    "   ...\n",
    "   \n",
    "- The finetuning process: \n",
    "\n",
    " - a 3-frame video sequence is passed into the pre-trained model for finetuning,\n",
    " \n",
    " - the finetuned model is saved into the `model` folder,\n",
    " \n",
    " - and after that the rest frames are passed into the finetuned model for frame prediction and anomaly scoring.\n",
    "\n",
    "- Each input is a 4-frame video sequence in the form of frame images, and the first 3 frames are used for the prediction of the 4-th frame.\n",
    "\n",
    "- The predicted frame is compared with the actual frame, and if the difference between the predicted frame and the actual frame is greater than a threshold (currently we use 0.6 at this stage), show this frame.\n",
    "\n",
    "\n",
    "## Suggestions for improvements\n",
    "\n",
    "The ways to improve the performance of our model are:\n",
    "\n",
    "- The training dataset is better to have more scenarios;\n",
    "\n",
    "- the current dataset is suffering from the lower resolution.\n",
    "\n",
    "### -- Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import ast\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import random\n",
    "import torch.utils.data\n",
    "import torchvision.utils as vutils\n",
    "import torch.backends.cudnn as cudn\n",
    "from torch.nn import functional as F\n",
    "from unet_parts import *\n",
    "# from scipy.misc import imsave\n",
    "import torch.nn as nn\n",
    "import ast\n",
    "import sys\n",
    "import imageio\n",
    "# from skimage import img_as_ubyte\n",
    "\n",
    "import cv2\n",
    "\n",
    "from rGAN import Generator, Discriminator\n",
    "from dataset import TrainingDataset\n",
    "from utils import createEpochData, roll_axis, loss_function, create_folder, prep_data, createEpochDataTest\n",
    "\n",
    "# load functions from the training script for the finetuning of the model\n",
    "from train import Load_Dataloader, overall_generator_pass, overall_discriminator_pass, meta_update_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_generator_pass_test(generator, discriminator, img, gt, valid):\n",
    "    # print(len(img), gt.shape)\n",
    "    recon_batch = generator(img)\n",
    "    recon_batch = (recon_batch-recon_batch.min()) / (recon_batch.max() - recon_batch.min())\n",
    "    gt = (gt-gt.min()) / (gt.max()-gt.min())\n",
    "    msssim, f1, psnr = loss_function(recon_batch, gt)\n",
    "    # print(msssim, f1, psnr)\n",
    "    \n",
    "    imgs = recon_batch.data.cpu().numpy()[0, :]\n",
    "    imgs = roll_axis(imgs)\n",
    "    \n",
    "    return imgs, psnr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Visualization functions for ground truth and predicted frame images\n",
    "\n",
    "We first visualize the first 3 frame video sequence (for the videos used in finetuning and testing/validation stage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def frame_visualization(img):\n",
    "    for frame in range(len(img)):\n",
    "        one_img = np.squeeze(img[frame])\n",
    "        one_img = one_img.cpu()\n",
    "        one_img = np.transpose(one_img, (1, 2, 0))\n",
    "        plt.imshow(one_img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "def pred_frame_visualization(img):\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Main test functions (including finetuning and validation/testing)\n",
    "- **Before running the following codes, you should update the `frame_path` to the test dataset you prepared as I mentioned in the beginning of this jupyter notebook.**\n",
    "- the threshold value for defining the anomaly is set as 0.85 at this stage for testing purposes, this value can be set to 0.9, 1.0, etc.\n",
    "- we use K-shot = 10 to test 10 videos, to test 100 videos just change K-shot = 100 in main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TEST SCRIPT\"\"\"\n",
    "def main(k_shots, num_tasks, adam_betas, gen_lr, dis_lr, model_folder_path):\n",
    "    torch.manual_seed(1)\n",
    "    batch_size = 1\n",
    "    \n",
    "    generator = Generator(batch_size=batch_size) \n",
    "    discriminator = Discriminator()\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    " \n",
    "\n",
    "    # define dataloader\n",
    "    tf = transforms.Compose([transforms.Resize((256,256)),transforms.ToTensor()])\n",
    "\n",
    "    create_folder(model_folder_path)\n",
    "    generator_path = os.path.join(model_folder_path, str.format(\"Generator_liyun4.pt\"))\n",
    "    discriminator_path = os.path.join(model_folder_path, str.format(\"Discriminator_liyun4.pt\"))\n",
    "    # generator_path = os.path.join(model_folder_path, str.format(\"Generator_Final.pt\"))\n",
    "    # discriminator_path = os.path.join(model_folder_path, str.format(\"Discriminator_Final.pt\"))\n",
    "    # for saving the fine-tuned model\n",
    "    generator_path_finetune = os.path.join(model_folder_path, str.format(\"Generator_finetuned_liyun1500_shtech.pt\"))\n",
    "    discriminator_path_finetune = os.path.join(model_folder_path, str.format(\"Discriminator_finetuned_liyun1500_shtech.pt\"))\n",
    "    \n",
    "    # load the pre-trained model\n",
    "    print('- start loading pre-trained model')\n",
    "#     script_module = torch.jit.load(generator_path)\n",
    "#     generator.load_state_dict(script_module.state_dict())\n",
    "    generator.load_state_dict(torch.load(generator_path))\n",
    "    discriminator.load_state_dict(torch.load(discriminator_path))\n",
    "    # if you use CPU\n",
    "#     generator.load_state_dict(torch.load(generator_path, map_location=torch.device('cpu')))\n",
    "#     discriminator.load_state_dict(torch.load(discriminator_path, map_location=torch.device('cpu')))\n",
    "    \n",
    "    print('- loading pretrained model done')\n",
    "    \n",
    "    # this path must be the video frames for testing purposes\n",
    "    # Lei uses the fake dataset as an example here\n",
    "    # frame_path = '/Users/leiwang/Desktop/fsl_AD-main/cameraTest'\n",
    "#     frame_path = '/Users/leiwang/Desktop/cameraTune/RedAsh4cams'\n",
    "\n",
    "    frame_path = r'C:\\Users\\liyun\\Desktop\\test_pic_normal'\n",
    "    \n",
    "    # the test dataloader\n",
    "    train_path_list = createEpochDataTest(frame_path, num_tasks, k_shots)\n",
    "    # print(frame_path)\n",
    "    train_dataloader = Load_Dataloader(train_path_list, tf, batch_size)\n",
    "\n",
    "    \n",
    "    # Meta-Validation\n",
    "    print ('\\n Meta Validation/Test \\n')\n",
    "\n",
    "    # forward pass\n",
    "    \n",
    "    for _, epoch_of_tasks in enumerate(train_dataloader):\n",
    "        \n",
    "        epoch_results = 'results'# .format(epoch+1)\n",
    "        create_folder(epoch_results)\n",
    "    \n",
    "        \n",
    "        gen_epoch_grads = []\n",
    "        dis_epoch_grads = []\n",
    "        \n",
    "        \n",
    "        for tidx, task in enumerate(epoch_of_tasks):\n",
    "            \n",
    "            # print ('\\n Meta finetuning \\n')\n",
    "#             print(task)\n",
    "            inner_optimizer_G = optim.Adam(generator.parameters(), lr=1e-4)\n",
    "            inner_optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "            \n",
    "            # for kidx, frame_sequence in enumerate(task[:k_shots]):\n",
    "            # using the first 3-5 frames for the finetuning\n",
    "            for kidx, frame_sequence in enumerate(task[:30]):\n",
    "                for ii in range(1):\n",
    "                    # Configure input\n",
    "                    img = frame_sequence[0]\n",
    "                    # print(len(img), ' ---')\n",
    "                    ## visualization of input frames\n",
    "\n",
    "                    # frame_visualization(img)\n",
    "\n",
    "                    gt = frame_sequence[1]\n",
    "                    # print('ground truth frame image for finetune')\n",
    "                    # frame_visualization(gt)\n",
    "\n",
    "                    img, gt, valid, fake = prep_data(img, gt)\n",
    "\n",
    "                    # Train Generator\n",
    "                    inner_optimizer_G.zero_grad()\n",
    "                    imgs, g_loss, recon_batch, loss, msssim = overall_generator_pass(generator, discriminator, img, gt, valid)\n",
    "                    img_path = os.path.join(epoch_results,'{}-fig-train{}.png'.format(tidx+1, kidx+1))\n",
    "                    # imsave(img_path , imgs)\n",
    "\n",
    "                    # imgs = imgs.astype(np.uint8)\n",
    "                    # imgs = (imgs-np.min(imgs))/(np.max(imgs) - np.min(imgs))\n",
    "                    imgs = cv2.normalize(imgs, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "                    # imageio.imwrite(img_path , img_as_ubyte(imgs))\n",
    "\n",
    "                    # print('--- prediction from finetune')\n",
    "                    # pred_frame_visualization(imgs)\n",
    "\n",
    "                    g_loss.backward()\n",
    "                    inner_optimizer_G.step()\n",
    "\n",
    "                    # Train Discriminator\n",
    "                    inner_optimizer_D.zero_grad()\n",
    "                    # Measure discriminator's ability to classify real from generated samples\n",
    "                    d_loss = overall_discriminator_pass(discriminator, recon_batch, gt, valid, fake)\n",
    "                    d_loss.backward()\n",
    "                    inner_optimizer_D.step()\n",
    "                    print (kidx, ' | ', ii, '- Reconstruction_Loss: {:.4f}, G_Loss: {:.4f}, D_loss: {:.4f},  msssim:{:.4f} '.format(loss.item(), g_loss, d_loss, msssim))\n",
    "            \n",
    "            # save the finetuned model\n",
    "            torch.save(generator.state_dict(), generator_path_finetune)\n",
    "            torch.save(discriminator.state_dict(), discriminator_path_finetune)\n",
    "            \n",
    "            print('model finetuning done! Now applying in testing...')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Main functions modified from training codes\n",
    "\n",
    "The following codes ignore the warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- start loading pre-trained model\n",
      "- loading pretrained model done\n",
      "----------- selected videos:  ['C:\\\\Users\\\\liyun\\\\Desktop\\\\test_pic_normal\\\\0']\n",
      "\n",
      " Meta Validation/Test \n",
      "\n",
      "0  |  0 - Reconstruction_Loss: 0.1514, G_Loss: 21.0885, D_loss: 6.8812,  msssim:0.0479 \n",
      "1  |  0 - Reconstruction_Loss: 0.1334, G_Loss: 16.9143, D_loss: 5.6582,  msssim:0.0405 \n",
      "2  |  0 - Reconstruction_Loss: 0.1192, G_Loss: 13.3475, D_loss: 4.5853,  msssim:0.0347 \n",
      "3  |  0 - Reconstruction_Loss: 0.1098, G_Loss: 10.3623, D_loss: 3.6532,  msssim:0.0301 \n",
      "4  |  0 - Reconstruction_Loss: 0.1057, G_Loss: 7.8608, D_loss: 2.8473,  msssim:0.0271 \n",
      "5  |  0 - Reconstruction_Loss: 0.1068, G_Loss: 5.7892, D_loss: 2.1591,  msssim:0.0252 \n",
      "6  |  0 - Reconstruction_Loss: 0.1079, G_Loss: 4.1109, D_loss: 1.5849,  msssim:0.0233 \n",
      "7  |  0 - Reconstruction_Loss: 0.1123, G_Loss: 2.7664, D_loss: 1.1362,  msssim:0.0222 \n",
      "8  |  0 - Reconstruction_Loss: 0.1196, G_Loss: 1.7727, D_loss: 0.8387,  msssim:0.0214 \n",
      "9  |  0 - Reconstruction_Loss: 0.1289, G_Loss: 1.1469, D_loss: 0.7046,  msssim:0.0225 \n",
      "10  |  0 - Reconstruction_Loss: 0.1399, G_Loss: 0.8197, D_loss: 0.6934,  msssim:0.0224 \n",
      "11  |  0 - Reconstruction_Loss: 0.1448, G_Loss: 0.6683, D_loss: 0.7331,  msssim:0.0219 \n",
      "12  |  0 - Reconstruction_Loss: 0.1465, G_Loss: 0.5992, D_loss: 0.7783,  msssim:0.0215 \n",
      "13  |  0 - Reconstruction_Loss: 0.1475, G_Loss: 0.5694, D_loss: 0.8074,  msssim:0.0210 \n",
      "14  |  0 - Reconstruction_Loss: 0.1481, G_Loss: 0.5614, D_loss: 0.8143,  msssim:0.0203 \n",
      "15  |  0 - Reconstruction_Loss: 0.1432, G_Loss: 0.5596, D_loss: 0.8060,  msssim:0.0196 \n",
      "16  |  0 - Reconstruction_Loss: 0.1386, G_Loss: 0.5680, D_loss: 0.7856,  msssim:0.0189 \n",
      "17  |  0 - Reconstruction_Loss: 0.1315, G_Loss: 0.5834, D_loss: 0.7577,  msssim:0.0180 \n",
      "18  |  0 - Reconstruction_Loss: 0.1237, G_Loss: 0.6082, D_loss: 0.7263,  msssim:0.0169 \n",
      "19  |  0 - Reconstruction_Loss: 0.1151, G_Loss: 0.6428, D_loss: 0.6950,  msssim:0.0161 \n",
      "20  |  0 - Reconstruction_Loss: 0.1077, G_Loss: 0.6875, D_loss: 0.6693,  msssim:0.0153 \n",
      "21  |  0 - Reconstruction_Loss: 0.0964, G_Loss: 0.7314, D_loss: 0.6524,  msssim:0.0153 \n",
      "22  |  0 - Reconstruction_Loss: 0.0879, G_Loss: 0.7752, D_loss: 0.6461,  msssim:0.0137 \n",
      "23  |  0 - Reconstruction_Loss: 0.0754, G_Loss: 0.8122, D_loss: 0.6464,  msssim:0.0131 \n",
      "24  |  0 - Reconstruction_Loss: 0.0658, G_Loss: 0.8453, D_loss: 0.6507,  msssim:0.0124 \n",
      "25  |  0 - Reconstruction_Loss: 0.0640, G_Loss: 0.8805, D_loss: 0.6552,  msssim:0.0120 \n",
      "26  |  0 - Reconstruction_Loss: 0.0545, G_Loss: 0.9038, D_loss: 0.6589,  msssim:0.0113 \n",
      "27  |  0 - Reconstruction_Loss: 0.0527, G_Loss: 0.9311, D_loss: 0.6610,  msssim:0.0111 \n",
      "28  |  0 - Reconstruction_Loss: 0.0501, G_Loss: 0.9552, D_loss: 0.6621,  msssim:0.0108 \n",
      "29  |  0 - Reconstruction_Loss: 0.0479, G_Loss: 0.9744, D_loss: 0.6625,  msssim:0.0107 \n",
      "model finetuning done! Now applying in testing...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if (len(sys.argv) == 8):\n",
    "        \"\"\"SYS ARG ORDER: \n",
    "        K_shots, num_tasks, adam_betas, generator lr, discriminator lr, total epochs, save model path\n",
    "        \"\"\"\n",
    "        k_shots = int(sys.argv[1])\n",
    "        num_tasks =  int(sys.argv[2])\n",
    "        adam_betas = ast.literal_eval(sys.argv[3])\n",
    "        gen_lr = float(sys.argv[4])\n",
    "        dis_lr = float(sys.argv[5])\n",
    "        model_folder_path = sys.argv[7]\n",
    "    else:\n",
    "        k_shots = 50\n",
    "        num_tasks = 1\n",
    "        adam_betas = (0.5, 0.999)\n",
    "        gen_lr = 2e-4\n",
    "        dis_lr = 1e-5\n",
    "        model_folder_path = \"model\"\n",
    "    main(k_shots, num_tasks, adam_betas, gen_lr, dis_lr, model_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
